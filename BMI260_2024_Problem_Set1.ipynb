{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIOMEDIN 260/RAD260: Problem Set 1 - Segmentation\n",
    "\n",
    "## Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Work\n",
    "\n",
    "Before we go any further: we encourage teamwork, and you can work by yourself or in pairs (2 people working together equally and submitting one notebook for the both of you). Who said radiology had to be lonely?\n",
    "\n",
    "**Person 1:**\n",
    "\n",
    "YOUR NAME HERE\n",
    "\n",
    "**Person 2:**\n",
    "\n",
    "YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iPython Notebooks: A lesson in reproducibility\n",
    "\n",
    "For those of you that are unfamiliar, iPython notebooks are interactive Python sessions that allow you to intersperse code, raw text, and markdown in a seamless manner. The next paragraph will teach you to use them.\n",
    "\n",
    "The words you are reading right now are modifiable. Double-click me to change the text that you're reading. These modifiable boxes are called cells. They will be used to write answers to our written questions. \n",
    "\n",
    "Double-click the box above me now, and go ahead and fill in your name (and your partner's as well, if applicable).\n",
    "\n",
    "Cells can contain executable code as well. If you're running the Jupyter Notebook program, You can change the type of cell by highlighting the cell of interest and going to `Cell -> Cell Type -> Markdown/Code`, and then clicking the desired cell type. If you're running the newer JupyterLab program, just click the drop-down menu next to the play, stop, and refresh icons above this window (along the top of the tabs detailing the current files that are open) and change the cell type directly there. \n",
    "\n",
    "You can add cells using Insert in the Jupyter Notebook program and the `'+'` icon in JupyterLab. \n",
    "\n",
    "The best thing about notebooks is that you can run small components of your code in one cell to make sure they work before putting them together to make a larger component. Make as many cells as you want to play and experiment, but please delete them if they are not part of your final submission. To delete a cell, highlight the cell and going to `Edit -> Delete Cells`, or use the keyboard shortcut of pressing the 'D' button twice. Make sure to not do this by mistake, but if you do, you can `\"Redo Cell Operation\"` from the respective menus in the different programs.\n",
    "\n",
    "Another nice thing about notebooks - the entire homework assignment is self-contained in this file! Please put all your functions and classes into the cells of this notebook, and please write clean code with at least some annotation to help us follow your thought process. Once you're done, export the notebook to a `.pdf` file.\n",
    "\n",
    "In research, it is important that code is readable and reproducible. Notebooks are a natural first step toward both goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run the code in the following cells by single clicking on them and then press CTRL+ENTER (SHIFT+ENTER on Mac) or click the play button in the menu bar (the one to the left of the stop button and to the right of the up and down arrows).\n",
    "\n",
    "The following first cell loads all the Python dependencies for this homework. It's OK if you get an error at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some packaged you might need, you can add more if you need them\n",
    "import os\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import skimage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you get an error like this?\n",
    "\n",
    "`### ModuleNotFoundError: No module named 'skimage'/'numpy'/'pydicom'?`\n",
    "\n",
    "It means you need to install some more dependencies. For example, PyDicom is not usually included.\n",
    "\n",
    "[Here](https://anaconda.org/conda-forge/pydicom) is how to install PyDicom given an Anaconda Python distribution, which we've asked you to get for this quarter, and [here](http://pydicom.readthedocs.io/en/stable/getting_started.html) is some PyDicom documentation which might be useful later on in the assignment.\n",
    "\n",
    "The other dependencies like scikit-image are installed [similarly](http://scikit-image.org/docs/dev/install.html).\n",
    "\n",
    "If you don't have an Anaconda Python distribution, you may have to use `sudo` like this:\n",
    "\n",
    "`$ sudo pip3 install pydicom`\n",
    "\n",
    "and likewise for `skimage`:\n",
    "\n",
    "`$ sudo pip3 install scikit-image`\n",
    "\n",
    "You can also install packages from within the Jupyter kernal. Try running the code in the cell below, which attempts to install NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "# or simply\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "We will be using data from the recent (2017) Kaggle Data Bowl challenge in this homework. You can download their data\n",
    "freely from [Kaggle](https://www.kaggle.com/c/data-science-bowl-2017) or on Canvas. We highly recommend downloading the “`CT_chest_scans.zip`” file on Canvas as this’ll give you full CT scans, but won’t take a day to download (total size < 1 GB).\n",
    "\n",
    "**Please download and unzip the data folder as soon as possible. Even the file for just the sample data is pretty big, coming in at ∼800 MB. You need the data to do the assignment. We will not accept your homework using any other data. Please try to download the data as soon as possible, and contact the TAs if you run into any problems. Do not wait until the last minute to download the data!**\n",
    "\n",
    "The data consist of sets of DICOM images that hold completely anonymized chest CT scans (see section “Reading in the Data”). DICOM (Digital Imaging and Communications in Medicine) is a standardized format for transmitting medical image data. Each DICOM file is composed of two parts: the image data as well as a header giving you a lot of metadata about the patient and specifics about the imaging parameters (e.g. space between slices).\n",
    "\n",
    "You can choose any of the patients in the Kaggle dataset (sample, training, or validation) to prototype your code, but we expect you to generalize your algorithm to work for at least ten different patients to show robustness. The more you do, the more you’ll prove your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure you can read in the data.\n",
    "\n",
    "In the cell below, replace the `scans_path` string variable with the (relative or absolute) path to the place where you put the data. Particularly, look for the folder that contains scans. Each scan is in its own folder and has a name, e.g. 0a0c32c9e08cc2ea76a71649de56be6d. `scans_path` should be the folder that contains these oddly named folders. Once you do this, you can run the cell below to look at all the metadata associated with that slice.\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "![dicom_fileds](figures/dicom_fields.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each chest CT scan is a folder with a long weird name like 0acbebb8d463b4b9ca88cf38431aac69 that within contains\n",
    "# many .dcm files with similar long weird names, assign the path to such folder to the variable scans_path \n",
    "\n",
    "scans_path = \"./CT_chest_scans\" # TODO: Change this to match where your data is located!\n",
    "list_of_scans = os.listdir(scans_path)\n",
    "list_of_scans.sort()\n",
    "\n",
    "# for figuring out the controls lets experiment with slice 20 of scan 5\n",
    "scan_num = 5\n",
    "print(f'Patient name: {list_of_scans[scan_num]}')\n",
    "scan_path = os.path.join(scans_path, list_of_scans[scan_num])\n",
    "list_of_slices = os.listdir(scan_path)\n",
    "slice_num = 20\n",
    "slice_path = os.path.join(scan_path, list_of_slices[slice_num])\n",
    "\n",
    "# read in the full path to the file as ds\n",
    "ds = dicom.read_file(slice_path) # you may have to use pydicom instead of dicom \n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a lot of metadata, right? Don't be scared. Next, let's actually look at the slice.\n",
    "\n",
    "We can see that there are many data fields in a DICOM file. There's a lot of patient information stored in a DICOM - name, birthdate, and so on. For obvious privacy reasons, this data has been completely anonymized. It’s pretty evident that our data probably didn’t come from Mr./Mrs. 0acbebb8d463b4b9ca88cf38431aac69 who was born on January 1st, 1900. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying images is fundamental to working in this field.\n",
    "Run the following cell to view the image associated with this very old person. Notice the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawimg = ds.pixel_array\n",
    "plt.imshow(rawimg, cmap='viridis')\n",
    "plt.show()\n",
    "print(type(rawimg), np.mean(ds.pixel_array), rawimg.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the image blue and green if CT slices are grayscale images?\n",
    "Previously we defined a `viridis` colormap for plotting. Let's change the colormap to one that matches our perception better and remove the pixel coordinate numbers for a cleaner visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawimg = ds.pixel_array\n",
    "plt.imshow(rawimg, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(type(rawimg), np.mean(ds.pixel_array), rawimg.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all set. On to the problem set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Segmentation is the process of dividing a given image into sections. This can be binary segmentation (if you wanted to separate the image into foreground and background) or a multilabel segmentation if you wanted to label many different parts of an image (see figures below for examples of each). Segmentation is an extremely interesting problem in image analysis, and it has yet to be solved perfectly. From an algorithmic point of view, we can approach the problem using tools such as graph cuts, watershed, flood fill / region growing, statistical set separation, etc... From a machine learning point of view, we can think of the problem as a pixel classification problem that can be solved with some supervised learning algorithm (assuming we had some ground truth training data) or an unsupervised learning algorithm (such as k-means clustering).\n",
    "\n",
    "<!-- <img src = \"figures/segmentation0.jpg\" > -->\n",
    "![segmentation0](figures/segmentation0.jpg)\n",
    "\n",
    "(a) Binary Segmentation - A binary segmentation of some grains of rice. We can use segmentation to create a binary mask that allows us to separate the rice (foreground) from the darker background. \n",
    "\n",
    "<!-- <img src = \"figures/segmentation1.jpg\" > -->\n",
    "![segmentation1](figures/segmentation1.png)\n",
    "\n",
    "(b) Multilabel Segmentation - A multilabel segmentation from a magnetic resonance image of the brain. We can see that different colors represent different sections of the brain. \n",
    "<!-- For example, the white matter on the left side of the brain is labeled with a different color than the white matter on the right side of the brain.  -->\n",
    "\n",
    "A lot of radiological image analysis begins with segmentation. For example, to quantitatively describe a lung nodule, you may want to collect data on its edge sharpness, its total volume, and/or the mean intensity of voxels in the nodule. However, to do this, we have to first be able to segment the nodule and cleanly define its boundaries. This can get extremely hairy. For example, in the chest, it might be hard to teach a program to segment a nodule in the lung from a seed voxel. It's important to make sure the nodule segmentation doesn’t bleed into the pericardium, as the voxel intensities for a nodule and the fleshy bits in the pericardium are very similar. We can try and restrict this through lung field segmentation (basically, we want to go through our chest CT scans and segment out the lungs) and thus define a region of interest, but this gets increasingly difficult as more and more problems pop up.\n",
    "\n",
    "This assignment will help you understand many core concepts taught in the lectures and hopefully\n",
    "integrate the lectures into a medically relevant application. The goals of this project include:\n",
    "\n",
    "a. Understanding medical image data format - specifically, DICOM.\n",
    "\n",
    "b. Understanding thresholding techniques - specifically, Otsu’s Threshold.\n",
    "\n",
    "c. Understanding morphological image analysis techniques, and expanding this understanding into 3D.\n",
    "\n",
    "d. Understanding image convolution and expanding it into efficient 3D.\n",
    "\n",
    "e. Understanding visualization in 3D.\n",
    "\n",
    "f. Embracing creativity and exploring image analysis.\n",
    "\n",
    "These can seem daunting tasks for a first timer, but we promise the final results will be amazingly cool! Stick with us, and as always, don't hesitate to ask for help.\n",
    "\n",
    "**The main goal of this assignment is to segment out the lungs, and only the lungs, from these CT slices, and then create a 3D rendering of the lungs to give yourself some way to qualitatively assess how good your segmentation is. Basically, given a chest CT series, you will be creating something like this:**\n",
    "\n",
    "<!-- <img src = \"figures/lung_field_segmentation.png\" > -->\n",
    "![lung_field_segmentation](figures/lung_field_segmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: (5 points)\n",
    "\n",
    "Some important DICOM fields like $RescaleIntercept$ and $RescaleSlope$ determine how the image pixel values should be interpreted. These metadata are critical for quantitative imaging methods like CT. The default raw pixel values are arbitrary units returned from the actual machine used and may differ based on the scanner manufacturer. We’d like to convert these raw values to Hounsfield units, which you learned about in lecture, in order to have some standardization among all of our CT scans. The conversion formula is:\n",
    "\n",
    "$$ \\textrm{Hounsfield Units} = RescaleSlope \\times \\textrm{Raw Image} + RescaleIntercept $$\n",
    "\n",
    "<!-- <img src=\"figures/houndsfield.png\"> -->\n",
    "\n",
    "![houndsfield](figures/houndsfield.png)\n",
    "\n",
    "### Deliverable: \n",
    "\n",
    "1) Read in the raw data for CT slice 120 for the patient above (`0acbebb8d463b4b9ca88cf38431aac69`)\n",
    "2) Convert the raw pixel values into Hounsfield units using the formula above. Hint: use the metadata in the DICOM file.\n",
    "3) Compute the max, min, mean, and standard deviation of both images (raw data and Hounsfield units)\n",
    "4) Compare them the summary statistics of both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 2 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: (25 points)\n",
    "\n",
    "As you may have noticed, we live in a 3D world. The next main part of your problem will be to go from all the individual slices of the CT scan (each one stored as a .dcm file) into a 3D volume. Basically, you’ll need to figure out some way to order these slices in the right order. Read in a few of the DICOM images from your patient, and try using different DICOM fields as a sorting key. You may find one of them works well in sorting the slices in the correct order. Try looking up what that field means in the DICOM standard. *Hint: you may want to initialize a blank 3D matrix called `volCT` or something.*\n",
    "\n",
    "Here is the pseudocode for one way (you are free to do your own) of creating this volume and filling it:\n",
    "\n",
    "<!-- <img src = \"figures/pseudocode.png\">  -->\n",
    "![pseudocode](figures/pseudocode.png)\n",
    "\n",
    "Visualize the NumPy volume you create from the stacks. Feel free to do this within Python with something like matplotlib.\n",
    "\n",
    "### Deliverable: \n",
    "\n",
    "Organize the 3D volume into an array and display 25 of the slices in correct order for us, like this:\n",
    "![lungs](figures/lung_order.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 4 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Display the organized CT volume ######\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i, j].imshow(volCT[:, :, (5*i+j)*5], cmap='gray')\n",
    "        ax[i, j].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.rcParams['figure.figsize'] = (30, 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (5 points)\n",
    "\n",
    "If you dont already know about numerical types, take a look [here](https://numpy.org/doc/stable/user/basics.types.html) and/or [here](https://www.tutorialspoint.com/numpy/numpy_data_types.htm).\n",
    "\n",
    "### Deliverable: \n",
    "\n",
    "1) What is the default numerical type used to store raw image data in DICOM files? Answer in the cell provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER TO SETP 3 PART 1) HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) In the coding cell below that one, convert all the pixels in your volume to a float32 type number between 0.0 and 1.0; output the min, max, and dtype (datatype) of the pixels in your volume before and after you convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. Your code to answer to 2) #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (10 points)\n",
    "\n",
    "Now that we have our 3D matrix of lung CT data, we can try to segment out our lung. We know from lecture that the pixel values of CT scans are given in Hounsfield units, where lower Hounsfield units correspond to low density materials (like air) that are not highly attenuative for X-rays and higher Hounsfield units correspond to highly attenuative materials, like bone.\n",
    "\n",
    "<!-- <img src = \"figures/histWithThresh.png\">  -->\n",
    "![histWithThresh](figures/histWithThresh.png)\n",
    "\n",
    "That red line separating the two groups of pixels is the [Otsu's threshold](https://en.wikipedia.org/wiki/Otsu%27s_method). We can find a nice separating value of our two modes with [this Otsu’s method](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_thresholding.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable:\n",
    "\n",
    "1) A histogram of the Hounsfield units from a typical CT scan will be significantly bimodal! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER TO STEP 4 PART 1 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the Otsu's threshold in your volume of pixels and plot the histogram of of your pixels with this line like the example figure. You will find these links ([1](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html), [2](https://matplotlib.org/1.2.1/examples/pylab_examples/histogram_demo.html), and [3](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.axvline.html)) useful if you have not done this before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 2 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (10 points)\n",
    "\n",
    "We can now take a look at the performance of Otsu’s threshold. We want all the pixels less than Otsu’s threshold since the lung is in the darker (low Hounsfield units) part of the image.\n",
    "\n",
    "<!-- <img src=\"figures/otsu.png\"> -->\n",
    "![otsu](figures/otsu_lungs.png)\n",
    "\n",
    "Despite the crude binary cutoff, we can see that this is really close to what we want for a final result, albeit with background and inne lung noise.\n",
    "\n",
    "### Deliverable:\n",
    "\n",
    "Display for us any one slice of the CT scan showing a binary image after applying thresholding using Otsu's method. Usually it's convention to show the area of interest after the segmentation in white and the background as black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 2 cells for this question, but only one is required #######\n",
    " \n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (30 points)\n",
    "\n",
    "Our dark lungs are surrounded by lighter-colored tissue, which is then again surrounded by darkness. Our lung segmentation is thus, interestingly, completely separated from the segmentation of the outside air. These two sets of binary connected components are completely unconnected. Can you think of a simple way of differentiating between the lungs and the air outside the body?\n",
    "\n",
    "Here's one way to do it - we assume that the outside air will always be touching the edge of the image and the lungs will not:\n",
    "\n",
    "<!-- <img src=\"figures/pseudocode2.png\"> -->\n",
    "![pseudocode2](figures/pseudocode2.png)\n",
    "\n",
    "However the segmentation is still rough around the edges. There are a few pixels of noise around the main lung (components that weren’t touching the edges, but still aren’t part of the lung). Luckily for us, these bits are quite small in size. We can therefore quickly filter them out based on their volume. We want to choose connected components that have a volume greater than some threshold $V$. You can choose $V$ arbitrarily (most noise will have pixel counts in the double digits or maybe low hundreds), as long as the lung volume should have a pixel count several orders of magnitude higher.\n",
    "\n",
    "Here would be a possible pseudo-algorithm for this task:\n",
    "\n",
    "<img src=\"figures/pseudocode3.png\">\n",
    "\n",
    "After this, we just want to smooth out the edges. The segmentation of our lung looks very ragged because it wasn’t able to pick out the lighter colored blood vessels in the lung. We can do this smoothing with a binary closing algorithm, provided by the `scikit-image` package (`skimage.morphology.binary_closing`). It works for both 2-dimensional and 3-dimensional binary matrices.\n",
    "\n",
    "### Deliverable:\n",
    "\n",
    "Implement the steps described here and display for us an ordered sample of the cuts from your boolean CT scan volume that has been smoothed, with only the pixels of the lung and trachea as 1/true, and everything else as 0/False,  like this:\n",
    "\n",
    "![final_segm](figures/final_segm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot the segmented binary lung volume ####\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i, j].imshow(volBW[:, :, (5*i+j)*5], cmap='gray')\n",
    "        ax[i, j].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.rcParams['figure.figsize'] = (30, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (5 points each, 15 points total)\n",
    "\n",
    "Show that the segmentation works on 3 more scans.\n",
    "\n",
    "## Deliverable:\n",
    "\n",
    "Same as Step 6, but with different scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 (Bonus 15 points, 5 points each)\n",
    "\n",
    "If you’ve finished everything above, you’ve already done a full fledged segmentation. However, for an extra challenge, you can try your hand at:\n",
    "\n",
    "1. Removing just the trachea so that your segmentation is just on the left and right lung without any of the trachea or bronchi. \n",
    "\n",
    "2. Separating out the two halves of the lung into left/right lungs.\n",
    "\n",
    "3. There are a ton of python packages that help you visualize the lung as a whole. We highly recommend looking up some marching cube algorithms, but the easier way to go would probably use something like ITK-SNAP or 3D Slicer. Simply [download ITK-SNAP](http://www.itksnap.org/pmwiki/pmwiki.php?n=Downloads.SNAP4) (We recommend version 4.0.2) or [3DSlicer](https://download.slicer.org). You can save all your 3D segmentation binary image slices as NIfTI file (.nii.gz or .nii) using the `nibabel` package. Note that the affine matrix can be an identity. Then, in ITK-SNAP or 3DSlicer load the file and activate the 3D visualizer. For the former its the `Toggle volume rendering` option, while for the latter its useful to import the file as a segmentation and use the `Create closed surface representation` option. This should give you a figure like what you see below:\n",
    "\n",
    "![3d_vis](figures/3d_vis_both.png)\n",
    "\n",
    "Show us that this works on at least two scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE IN HERE. You can have up to 5 cells for this question, but only one is required #######\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Correcting Non-Uniform Illumination - Rice Image](https://www.mathworks.com/help/images/correcting-nonuniform-illumination.html)\n",
    "\n",
    "[Segmentation - Brain Image](https://analyticsindiamag.com/torchio-3d-medical-imaging/)\n",
    "\n",
    "We thank Darvin Yi and Laura Bravo Sánchez for the content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
